{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Predictive Maintenance\n",
    "\n",
    "This notebook is part of [*Hands-on Machine Learning for IoT*](https://github.com/pablodecm/datalab_ml_iot) tutorial by Pablo de Castro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tools\n",
    "\n",
    "This notebook will use the following Python 3\n",
    "libraries for data analytics and machine learning:\n",
    "- pandas\n",
    "- numpy\n",
    "- matplotlib\n",
    "- scikit-learn\n",
    "- keras/tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset that will be used, which was published by NASA [[3](#References)],\n",
    "consist on simulated turbojet engine degradation\n",
    "under different combinations of operational conditions.\n",
    "\n",
    "The main task is to predict when the engine is about to fail before it fails.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/airbus-turbofan.jpg\" height=\"50%\" style=\"max-width: 50%\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!wget https://ti.arc.nasa.gov/c/6/ -O data/CMAPSSData.zip\n",
    "!unzip -o data/CMAPSSData.zip -d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# this converts the encoding\n",
    "!iconv -f ISO-8859-1 -t UTF-8//TRANSLIT data/readme.txt -o data/readme_enc.txt\n",
    "!cat data/readme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# train and test data are simple space separated values\n",
    "!head -5 data/train_FD001.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# the test truth is given as the number of steps to failure\n",
    "# for each engine run in the test set (100 in total)\n",
    "!head -5 data/RUL_FD001.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# load data (only gonna use FD001 dataset)\n",
    "train_df = pd.read_csv('data/train_FD001.txt', sep=\" \", header=None)\n",
    "test_df  = pd.read_csv('data/test_FD001.txt', sep=\" \", header=None)\n",
    "print(\"train shape: \", train_df.shape, \"test shape: \", test_df.shape)\n",
    "# lets have a look at basic descriptive statistics\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# we will remove columns 26 and 27 because of the NaNs\n",
    "train_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "test_df.drop(test_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "print(\"train shape: \", train_df.shape, \"test shape: \", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# the files did not contain headers\n",
    "# we can create them based on the documentation\n",
    "target_var = ['target_RUL']\n",
    "index_columns_names =  [\"UnitNumber\",\"Cycle\"]\n",
    "op_settings_columns = [\"Op_Setting_\"+str(i) for i in range(1,4)]\n",
    "sensor_columns =[\"Sensor_\"+str(i) for i in range(1,22)]\n",
    "column_names = index_columns_names + op_settings_columns + sensor_columns\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# name columns\n",
    "train_df.columns = column_names\n",
    "test_df.columns = column_names\n",
    "\n",
    "# now the dataset looks better, e.g. the first unit\n",
    "train_df[train_df.UnitNumber == 1].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Remaining Useful Life (RUL)\n",
    "\n",
    "The training data consists time-series for the engine sensors\n",
    "for each cycle (i.e. timestep) until failure which happens\n",
    "after the last time step.\n",
    "\n",
    "Thus, the Remaining Useful Life (RUL), i.e. time until the\n",
    "engine breaks, can be calculated based on the maximum cycle\n",
    "of each unit present in the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# find the last cycle per unit number\n",
    "max_cycle = train_df.groupby('UnitNumber')['Cycle'].max().reset_index()\n",
    "max_cycle.columns = ['UnitNumber', 'MaxOfCycle']\n",
    "# merge the max cycle back into the original frame\n",
    "train_df = train_df.merge(max_cycle, left_on='UnitNumber', right_on='UnitNumber', how='inner')\n",
    "# calculate RUL for each row\n",
    "target_RUL = train_df[\"MaxOfCycle\"] - train_df[\"Cycle\"]\n",
    "# add columns and remove MaxOfCycle\n",
    "train_df[\"target_RUL\"] = target_RUL\n",
    "train_df = train_df.drop(\"MaxOfCycle\", axis=1)\n",
    "# check that it worked for unit 1\n",
    "train_df[train_df.UnitNumber == 1].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The test data does not correspond to running until failure\n",
    "as discussed in the dataset documentation, the RUL at the last\n",
    "step is instead provided on an additional file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get truth RUL\n",
    "truth_df = pd.read_csv('data/RUL_FD001.txt', sep=\" \", header=None)\n",
    "truth_df.drop(truth_df.columns[[1]], axis=1, inplace=True)\n",
    "# UnitNumber based\n",
    "truth_df.columns = [\"RUL_after_last\"]\n",
    "truth_df['UnitNumber'] =  truth_df.index + 1\n",
    "# find the last cycle per unit number in test set\n",
    "max_cycle = test_df.groupby('UnitNumber')['Cycle'].max().reset_index()\n",
    "max_cycle.columns = ['UnitNumber', 'MaxOfCycle']\n",
    "max_cycle['MaxOfCycle'] = max_cycle['MaxOfCycle'] + truth_df[\"RUL_after_last\"]\n",
    "# merge the max cycle back into the original frame\n",
    "test_df = test_df.merge(max_cycle, left_on='UnitNumber', right_on='UnitNumber', how='inner')\n",
    "# calculate RUL for each row\n",
    "target_RUL = test_df[\"MaxOfCycle\"] - test_df[\"Cycle\"]\n",
    "# add columns and remove MaxOfCycle\n",
    "test_df[\"target_RUL\"] = target_RUL\n",
    "test_df = test_df.drop(\"MaxOfCycle\", axis=1)\n",
    "# check that it worked for unit 1\n",
    "test_df[test_df.UnitNumber == 1].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Defining the Problem\n",
    "\n",
    "**The first step before starting with ML for a IoT application (or any problem for that matter)\n",
    "is to understand well the talk at hand.**\n",
    "\n",
    "Predictive maintenance is about having accurate predictions (based on sensors or performances)\n",
    "of when a machine or a industrial setup will fail and how to schedule costly maintenance\n",
    "intelligently and reduce operating costs.\n",
    "\n",
    "The most common problem definitions for predictive maintenance are: \n",
    "\n",
    "- Regression (**included here**): Predict the Remaining Useful Life (RUL)or Time to Failure (TTF).\n",
    "- Binary classification (**included here**): Predict if an asset will fail within certain time frame (e.g. days).\n",
    "- Multi-class classification (**not included here**): Predict if an asset will fail in different time windows or due to different failure models\n",
    "\n",
    "\n",
    "The techniques are those also used for general time-series\n",
    "forecasting but at application time we do not have access to the target value at the current or\n",
    "previous time step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature Exploration\n",
    "\n",
    "Independently on the chosen problem, it is always recommended\n",
    "to interactively explore the variables to be considered\n",
    "in the predictive modelling problem.\n",
    "\n",
    "It is important to not only consider the data available\n",
    "in the training set but also that expected in the test\n",
    "or production environment. Alternatively we can\n",
    "incur in:\n",
    "- **target leakage**: use information that has predictive power\n",
    "as input of the model during training but will no be available\n",
    "in production or for the real data.\n",
    "- **domain mismatch**: if the training and test/production data\n",
    "are different the trained models might not perform well in\n",
    "the real word scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# we will consider all features except the UnitNumber and the target\n",
    "basic_features = train_df.columns.difference([\"UnitNumber\",\"target_RUL\"])\n",
    "print(\"basic_features: \",basic_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Exercise: Plot the Sensor Data for given Unit\n",
    "\n",
    "For a given unit of the training set, plot the time-series of some of the sensor data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space for exercise solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Exercise: Compare the Distribution of Variables in the Train and Test sets\n",
    "\n",
    "Compare graphically the distribution of some of the\n",
    "input variables (e.g. 'Cycle', 'Op_Setting_1' or sensor data) for the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space for exercise solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Exercise: Compare the Distrubution of the Target in the Train and Test sets\n",
    "\n",
    "Compare graphically the distribution of the target RUL in the train and test sets.\n",
    "Will this affect the training? How could it be avoided (see reference [9](#References))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space for exercise solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature Transformations and Engineering\n",
    "\n",
    "Another important step, particularly when dealing with\n",
    "most techniques other than Deep Learning, is *feature\n",
    "preprocessing/scaling* and *feature engineering*.\n",
    "\n",
    "Feature preprocessing/scaling can facilitate model training by\n",
    "scaling the features to dimensionless values based on the properties\n",
    "of the dataset.\n",
    "\n",
    "Feature engineering, the definition of new variables based\n",
    "on those available, is particularly important for time-series\n",
    "data when we want to\n",
    "make a prediction for each timestep as is the case for all the\n",
    "problems considered in this notebook.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# we will use the Standard Scaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "features = basic_features\n",
    "X_unscaled = train_df[features].astype('float64')\n",
    "X = pd.DataFrame(scaler.fit_transform(X_unscaled),\n",
    "                 columns = features,\n",
    "                 index = train_df.index)\n",
    "y = train_df[\"target_RUL\"]\n",
    "\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X_test_unscaled = test_df[features].astype('float64')\n",
    "X_test = pd.DataFrame(scaler.transform(X_test_unscaled),\n",
    "                      columns = features,\n",
    "                      index = test_df.index)\n",
    "y_test = test_df[\"target_RUL\"]\n",
    "\n",
    "X_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RUL Prediction as a Regression Task\n",
    "\n",
    "**How many more cycles an in-service engine will last before it fails?**\n",
    "\n",
    "The task of predicting the Remaining Useful Life (RUL)\n",
    "can be casted as a regression task in the context of machine\n",
    "learning. RUL can also be referred as Time to Failure (TTF).\n",
    "\n",
    "\n",
    "The goal in a regression problem is to find a function\n",
    "$f_R(\\boldsymbol{x})$ that approximates the true target $y$. The\n",
    "target in this problem will be the RUL, while $\\boldsymbol{x}$\n",
    "will be the input features (e.g. sensor readings).\n",
    "\n",
    "To measure the goodness of our regression function, we need\n",
    "to define and score function, which will be also the loss\n",
    "function for some of the machine learning techniques considered.\n",
    "We will be considering the mean squared error:\n",
    "$$ \\textrm{MSE} = \\sum_{i=1}^{n} (y - f_R(\\boldsymbol{x}))^2$$\n",
    "which is one of the most common losses used for regression,\n",
    "depending on the problem alternative loss definitions might\n",
    "be more beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Baseline and Feature Importance\n",
    "\n",
    "Sometimes is useful to train a simple model for the task at hand to\n",
    "get a baseline performance. A random forest has the advantage that\n",
    "it can also provide a list of the relative importance of the\n",
    "different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "rf = ensemble.RandomForestRegressor()\n",
    "simple_rf = ensemble.RandomForestRegressor(n_estimators = 200, max_depth = 15)\n",
    "simple_rf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "y_pred = simple_rf.predict(X)\n",
    "print(\"[Train] Simple RF Mean Squared Error: \", mean_squared_error(y, y_pred))\n",
    "print(\"[Train] Simple RF Mean Absolute Error: \", mean_absolute_error(y, y_pred))\n",
    "print(\"[Train] Simple RF r-squared: \", r2_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = simple_rf.predict(X_test)\n",
    "print(\"[Test] Simple RF Mean Squared Error: \", mean_squared_error(y_test, y_test_pred))\n",
    "print(\"[Test] Simple RF Mean Absolute Error: \", mean_absolute_error(y_test, y_test_pred))\n",
    "print(\"[Test] Simple RF r-squared: \", r2_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# graph feature importance\n",
    "import matplotlib.pyplot as plt\n",
    "importances = simple_rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "feature_names = X.columns    \n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "plt.title(\"Feature ranking\", fontsize = 20)\n",
    "plt.bar(range(X.shape[1]), importances[indices], color=\"b\", align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices) #feature_names, rotation='vertical')\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.ylabel(\"importance\", fontsize = 18)\n",
    "plt.xlabel(\"index of the feature\", fontsize = 18)\n",
    "plt.show()\n",
    "# list feature importance\n",
    "important_features = pd.Series(data=simple_rf.feature_importances_,index=X.columns)\n",
    "important_features.sort_values(ascending=False,inplace=True)\n",
    "print(important_features.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Cross Validation and Hyper-Parameters\n",
    "\n",
    "By means of cross validation and hyper-parameter search, we\n",
    "can try to obtain a better regression model based on RandomForest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold, GridSearchCV\n",
    "\n",
    "rf = ensemble.RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "# to avoid having same UnitNumber in both sets\n",
    "cv = GroupKFold(5)\n",
    "\n",
    "param_grid = { \"min_samples_leaf\" : [2, 10, 25, 50, 100],\n",
    "               \"max_depth\" : [7, 8, 9, 10, 11, 12]}\n",
    "\n",
    "optimized_rf = GridSearchCV(estimator=rf,\n",
    "                            cv = cv,\n",
    "                            param_grid=param_grid,\n",
    "                            scoring='neg_mean_squared_error',\n",
    "                            verbose = 1,\n",
    "                            n_jobs = -1)\n",
    "\n",
    "optimized_rf.fit(X, y, groups = train_df.UnitNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "y_test_pred = optimized_rf.predict(X_test)\n",
    "print(\"[Test] Optimized RF Mean Squared Error: \", mean_squared_error(y_test, y_test_pred))\n",
    "print(\"[Test] Optimized RF Mean Absolute Error: \", mean_absolute_error(y_test, y_test_pred))\n",
    "print(\"[Test] Optimized RF r-squared: \", r2_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Other Models: Gradient Boosting\n",
    "\n",
    "Gradient Boosted regression and classification are some of the\n",
    "best performing model for a variety of tasks, let us see if they\n",
    "can also be applied to this dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gb = GradientBoostingRegressor()\n",
    "\n",
    "# to avoid having same UnitNumber in both sets\n",
    "cv = GroupKFold(5)\n",
    "\n",
    "param_grid = { \"alpha\" : [.75, .9],\n",
    "               \"n_estimators\" : [500],\n",
    "               \"learning_rate\" :  [.01],\n",
    "                \"max_depth\" : [4, 5, 6]\n",
    "             }\n",
    "\n",
    "optimized_gb= GridSearchCV(estimator=gb,\n",
    "                            cv = cv,\n",
    "                            param_grid=param_grid,\n",
    "                            scoring='neg_mean_squared_error',\n",
    "                            verbose = 1,\n",
    "                            n_jobs = -1)\n",
    "\n",
    "optimized_gb.fit(X, y, groups = train_df.UnitNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = optimized_gb.predict(X_test)\n",
    "print(\"[Test] Optimized GB Mean Squared Error: \", mean_squared_error(y_test, y_test_pred))\n",
    "print(\"[Test] Optimized GB Mean Absolute Error: \", mean_absolute_error(y_test, y_test_pred))\n",
    "print(\"[Test] Optimized GB r-squared: \", r2_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Exercise: Train Another Cross Validated Model\n",
    "\n",
    "Based on the previous examples, use cross validation to find a good\n",
    "set of hyper-parameters and benchmark on the test dataset another scikit-learn regression model ([see documentation](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning)), for example choose between:\n",
    "1. `sklearn.svm.SVR`: [Epsilon-Support Vector Regression](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR)\n",
    "2. `sklearn.neural_network.MLPRegressor` : [Multilayer Perceptron](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor)\n",
    "3. **Advanced Track**: use `GradientBoostingRegressor` but add some engineering features in order to try to improve the previous result (e.g. moving averages and standard deviations of features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space for the exercise solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Recurrent Neural Network Model for Regression\n",
    "\n",
    "All the previous machine learning model can only take tabular data, i.e.\n",
    "features at a given timestep $t$. Information about the previous time\n",
    "step can only be include by means of clever feature engineering.\n",
    "\n",
    "Deep Neural Networks, in particular Recurrent architectures such\n",
    "as LSTM [[6,7]](#References), can be used\n",
    "to automatically learn feature transformation on sequence data\n",
    "for a given task.\n",
    "\n",
    "Here, a basic example of training a LSTM based model for\n",
    "predicting the RUL is provided, based on\n",
    "the Keras-based implementation from [[2]](#References)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Activation\n",
    "\n",
    "# define path to save model\n",
    "model_path = 'regression_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# pick a large window size of 50 cycles\n",
    "seq_length = 50\n",
    "\n",
    "# generator to reshape features into (samples, time steps, features) \n",
    "def gen_sequence(unit_number_df, seq_length = seq_length, seq_cols = features):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
    "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
    "    we can use shorter ones \"\"\"\n",
    "    # for one unit number I put all the rows in a single matrix\n",
    "    data_matrix = unit_number_df[seq_cols].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # Iterate over two lists in parallel.\n",
    "    # For example UnitNumber 1 have 192 rows and sequence_length is equal to 50\n",
    "    # so zip iterate over two following list of numbers\n",
    "    # 0 50 -> from row 0 to row 50\n",
    "    # 1 51 -> from row 1 to row 51\n",
    "    # 2 52 -> from row 2 to row 52\n",
    "    # ...\n",
    "    # 111 191 -> from row 111 to 191\n",
    "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
    "        yield data_matrix[start:stop, :]\n",
    "\n",
    "# test, it has to be 142 (192-seq_length)\n",
    "val=list(gen_sequence(X[train_df['UnitNumber']==1]))\n",
    "print(len(val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# generator for the sequences\n",
    "# transform each UnitNumber\n",
    "# of the train dataset in a sequence\n",
    "seq_gen = (list(gen_sequence(X[train_df['UnitNumber']== un])) \n",
    "           for un in train_df['UnitNumber'].unique())\n",
    "\n",
    "# convert sequences and convert to numpy array\n",
    "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
    "print(seq_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# function to generate labels\n",
    "def gen_labels(unit_number_df, seq_length = seq_length,\n",
    "               label = [\"target_RUL\"]):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
    "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
    "    we can use shorter ones \"\"\"\n",
    "    # For one id I put all the labels in a single matrix.\n",
    "    # For example:\n",
    "    # [[1]\n",
    "    # [4]\n",
    "    # [1]\n",
    "    # [5]\n",
    "    # [9]\n",
    "    # ...\n",
    "    # [200]] \n",
    "    data_matrix = unit_number_df[label].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # I have to remove the first seq_length labels\n",
    "    # because for one id the first sequence of seq_length size have as target\n",
    "    # the last label (the previus ones are discarded).\n",
    "    # All the next id's sequences will have associated step by step one label as target.\n",
    "    return data_matrix[seq_length:num_elements, :]\n",
    "\n",
    "# generate labels\n",
    "label_gen = [gen_labels(train_df[train_df['UnitNumber'] == un]) \n",
    "             for un in train_df['UnitNumber'].unique()]\n",
    "\n",
    "label_array = np.concatenate(label_gen).astype(np.float32)\n",
    "label_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Next, we build a deep network. \n",
    "# The first layer is an LSTM layer with 100 units followed by another LSTM layer with 50 units. \n",
    "# Dropout is also applied after each LSTM layer to control overfitting. \n",
    "# Final layer is a Dense output layer with single unit and linear activation since this is a regression problem.\n",
    "nb_features = seq_array.shape[2]\n",
    "nb_out = label_array.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(\n",
    "         input_shape=(seq_length, nb_features),\n",
    "         units=100,\n",
    "         return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(\n",
    "          units=50,\n",
    "          return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=nb_out))\n",
    "model.add(Activation(\"linear\"))\n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop',metrics=['mae'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# fit the network\n",
    "history = model.fit(seq_array, label_array, epochs=100, batch_size=200, validation_split=0.05, verbose=2,\n",
    "          callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'),\n",
    "                       keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,1, figsize=(10,10))\n",
    "\n",
    "axs[0].plot(history.history['mean_absolute_error'])\n",
    "axs[0].plot(history.history['val_mean_absolute_error'])\n",
    "axs[0].set_title('model MAE')\n",
    "axs[0].legend(['train', 'test'], loc='upper right')\n",
    "axs[0].set_xlabel('epoch')\n",
    "\n",
    "\n",
    "axs[1].plot(history.history['loss'])\n",
    "axs[1].plot(history.history['val_loss'])\n",
    "axs[1].set_title('model MSE (loss)')\n",
    "axs[1].legend(['train', 'test'], loc='upper right')\n",
    "axs[1].set_xlabel('epoch')\n",
    "\n",
    "y_pred = model.predict(seq_array,verbose=1, batch_size=200)\n",
    "y = label_array\n",
    "print(\"[Train] LSTM Regression Mean Squared Error: \", mean_squared_error(y, y_pred))\n",
    "print(\"[Train] LSTM Regression Mean Absolute Error: \", mean_absolute_error(y, y_pred))\n",
    "print(\"[Train] LSTM Regression r-squared: \", r2_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Test Set Evaluation\n",
    "\n",
    "The real world performance is better evaluate over the set\n",
    "of data that has not been used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the last sequence for each id in the test data\n",
    "sequence_cols = features\n",
    "seq_array_test_last = [X_test[test_df['UnitNumber']==un][sequence_cols].values[-seq_length:] \n",
    "                       for un in test_df['UnitNumber'].unique() if len(test_df[test_df['UnitNumber']== un]) >= seq_length]\n",
    "\n",
    "seq_array_test_last = np.asarray(seq_array_test_last).astype(np.float32)\n",
    "\n",
    "# similarly, we pick the labels\n",
    "y_mask = [len(test_df[test_df['UnitNumber']==un]) >= seq_length for un in test_df['UnitNumber'].unique()]\n",
    "label_array_test_last = test_df.groupby('UnitNumber')['target_RUL'].nth(-1)[y_mask].values\n",
    "label_array_test_last = label_array_test_last.reshape(label_array_test_last.shape[0],1).astype(np.float32)\n",
    "\n",
    "print(seq_array_test_last.shape)\n",
    "print(label_array_test_last.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# if best iteration's model was saved then load and use it\n",
    "if os.path.isfile(model_path):\n",
    "    estimator = load_model(model_path)\n",
    "\n",
    "    y_pred_test = estimator.predict(seq_array_test_last, batch_size=200)\n",
    "    y_true_test = label_array_test_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"[Test] LSTM Regression Mean Squared Error: \", mean_squared_error(y_true_test, y_pred_test))\n",
    "print(\"[Test] LSTM Regression Mean Absolute Error: \", mean_absolute_error(y_true_test, y_pred_test))\n",
    "print(\"[Test] LSTM Regression r-squared: \", r2_score(y_true_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot in blue color the predicted data and in green color the\n",
    "# actual data to verify visually the accuracy of the model.\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(y_pred_test, color=\"blue\")\n",
    "ax.plot(y_true_test, color=\"green\")\n",
    "ax.set_title('prediction on unseem data')\n",
    "ax.set_ylabel('RUL value')\n",
    "ax.set_xlabel('row')\n",
    "ax.legend(['predicted', 'actual data'], loc='lower left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Predict Failures using Classification\n",
    "\n",
    "**Will the unit fail within a certain time-frame (i.e. number of cycles)?**\n",
    "\n",
    "This can be though of a classification problem,\n",
    "where the boolean target is whether the unit will fail\n",
    "within the next $n$ timesteps (e.g. 15 cycles).\n",
    "\n",
    "The goal in a (soft) classification problem is to find a function\n",
    "$f_C(\\boldsymbol{x})$ that approximates probabilities of belonging\n",
    "to a set of classes or categories $y$. The\n",
    "target in this problem will be the whether the engine\n",
    "will fail in the next 15 timesteps, while $\\boldsymbol{x}$\n",
    "will be the input features (e.g. sensor readings).\n",
    "\n",
    "To measure the goodness of our classification function, we need\n",
    "to define and score function, which will be also the loss\n",
    "function for the machine learning techniques considered.\n",
    "We will be considering the binary cross entropy:\n",
    "$$ \\textrm{BCE} = \\sum_{i=1}^{n} (y \\log \\left(f_C(\\boldsymbol{x})\\right) - (1-y) y \\log \\left(1-f_C(\\boldsymbol{x}) \\right )$$\n",
    "which is one of the most common losses used for binary\n",
    "classification, and could be extended also to the \n",
    "multiclass/multilabel problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# we can keep the same input features, we have only to compute the\n",
    "# new target (we will only use the train set for classification)\n",
    "cycles = 15\n",
    "train_df['Target_15_Cycles'] = np.where(train_df['target_RUL'] <= cycles, 1, 0 )\n",
    "\n",
    "y_clf = train_df[\"Target_15_Cycles\"]\n",
    "\n",
    "X_clf_train, X_clf_test, y_clf_train, y_clf_test = train_test_split(X, y_clf, test_size=0.2, random_state=1234)\n",
    "\n",
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Gradient Boosting for Classification\n",
    "\n",
    "Similarly to what was done for the regression problem and given\n",
    "the good performance observed for the Gradient Boosting model,\n",
    "we will train a gradient boosted model for classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_clf = GradientBoostingClassifier()\n",
    "\n",
    "# to avoid having same UnitNumber in both sets\n",
    "cv = KFold(3)\n",
    "\n",
    "param_grid = { \"n_estimators\" : [500],\n",
    "               \"learning_rate\" :  [.01],\n",
    "                \"max_depth\" : [6]\n",
    "             }\n",
    "\n",
    "optimized_gb_clf = GridSearchCV(estimator=gb_clf,\n",
    "                            cv = cv,\n",
    "                            param_grid=param_grid,\n",
    "                            verbose = 1,\n",
    "                            n_jobs = -1)\n",
    "\n",
    "optimized_gb_clf.fit(X_clf_train, y_clf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_test_clf_proba = optimized_gb_clf.predict_proba(X_clf_test)[:, 1]\n",
    "y_test_clf_pred = optimized_gb_clf.predict(X_clf_test)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_clf_test,y_test_clf_pred))\n",
    "print(\"Gradient Boosting Classifier Accuracy: \"+\"{:.1%}\".format(accuracy_score(y_clf_test,y_test_clf_pred)));\n",
    "print(\"Gradient Boosting Classifier Precision: \"+\"{:.1%}\".format(precision_score(y_clf_test,y_test_clf_pred)));\n",
    "print(\"Gradient Boosting Classifier Recall: \"+\"{:.1%}\".format(recall_score(y_clf_test,y_test_clf_pred)));\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_clf_test,y_test_clf_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_clf_test,y_test_clf_proba)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_title('Receiver Operating Characteristic Curve')\n",
    "ax.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "ax.legend(loc = 'lower right')\n",
    "ax.plot([0, 1], [0, 1],'r--')\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1])\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_xlabel('False Positive Rate')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Advanced Exercise: Recurrent Neural Network Model for Classification\n",
    "\n",
    "Based on the previous regression example and the classification\n",
    "target, train and evaluate a model for predictive\n",
    "maintenance classification using a LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space for the exercise solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more space for the exercise solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "This notebook is heavily based on these resources on the topic:\n",
    "\n",
    "- [1] [*Predictive Maintenance Template*](https://gallery.azure.ai/Collection/Predictive-Maintenance-Template-3)  by Microsoft Azure ML Team \n",
    "- [2] [*Predictive Maintenance using LSTM*](https://github.com/umbertogriffo/Predictive-Maintenance-using-LSTM) by Umberto Griffo\n",
    "- [3] [*Predictive Maintenance ML (IIOT)*](https://www.kaggle.com/billstuart/predictive-maintenance-ml-iiot) by Bill Stuart\n",
    "\n",
    "The dataset used was provided by NASA:\n",
    "\n",
    "- [4] A. Saxena and K. Goebel (2008). [*Turbofan Engine Degradation Simulation Data Set*](https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/#turbofan), NASA Ames Prognostics Data Repository, NASA Ames Research Center, Moffett Field, CA\n",
    "\n",
    "\n",
    "Other resources and tutorials on Predictive Maintenance:\n",
    "\n",
    "- [5] [*Predictive Maintenance Modelling Guide*](https://gallery.azure.ai/Collection/Predictive-Maintenance-Implementation-Guide-1) by Fidan Boyly Uz\n",
    "\n",
    "Two good really good posts on the concept and usefulness of Recurrent Neural Networks for sequence data:\n",
    "\n",
    "- [6] [*The Unreasonable Effectiveness of Recurrent Neural Networks*](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy\n",
    "\n",
    "- [7] [*Understanding LSTM Networks*](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Christopher Olah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This recent papers (with code) use this dataset in combination\n",
    "with more advanced Deep Learning architectures and data augmentation\n",
    "to achieve state of the art (SOTA):\n",
    "\n",
    "- [8] S. Theng et al. [*Long short-term\n",
    "memory network for remaining useful life estimation*](https://ieeexplore.ieee.org/document/7998311) in Proc. IEEE International Conference on Prognostics and Health\n",
    "\n",
    "- [9] L. Jayasinghe et al. [*Temporal Convolutional Memory Networks for\n",
    "Remaining Useful Life Estimation of Industrial Machinery*](https://github.com/LahiruJayasinghe/RUL-Net) in IEEE International Conference on Industrial Technology (ICIT2019)\n",
    "\n",
    "Not many books on this topic, there is a book on ML for IOT (free 1 month subscription online), yet the\n",
    "advanced ML chapter do not include specific IoT applications:\n",
    "\n",
    "- [10] [Hands-On Artificial Intelligence for IoT](https://www.packtpub.com/big-data-and-business-intelligence/hands-artificial-intelligence-iot?utm_source=github&utm_medium=repository&utm_campaign=9781788836067) by Amita Kapoor (2019)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
