{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Internet of Wands (WIP)\n",
    "\n",
    "This notebook is part of [*Practical Data Science for IoT*](https://github.com/pablodecm/datalab_ml_iot) tutorial by Pablo de Castro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview of Use Case\n",
    "\n",
    "The aim of this example is to demonstrate an end-to-end\n",
    "example of a machine learning for a (consumer) IoT application\n",
    "and remark the main challenges associated.\n",
    "\n",
    "**The use case chosen is a imaginary application where smartphones\n",
    "devices act as magic wands and we want to make a spell recognition\n",
    "system, which will be referred as Internet of Wands (IoW).**\n",
    "\n",
    "We will be focussing on how could we can collect and process \n",
    "for training and evaluating a model for such an application. We will also\n",
    "discuss how we could deploy to production.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Important Remark\n",
    "\n",
    "Most of the discussion and technology choices to follow are\n",
    "not unique of this application and could be readily applied\n",
    "to use cases  such as:\n",
    "- Human Activity Recognition with Wearables (e.g. running, lying down, driving or sitting)\n",
    "- Elderly fall/accident/emergency alert system\n",
    "- Possible Consumer Applications, for example:\n",
    "    - Gym Repetition Counter: identify exercise and count the reps based on wearables\n",
    "    - Parkinson Disease Early Detection system\n",
    "- Also broadly related with distributed training applications such a self-driving cars (e.g. Tesla Object recognition model [[1]](#References))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Important Aspects\n",
    "\n",
    "We will be discussing many aspects of the data cycle in supervised ML workflows for IoT, such as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Training Data Collection*:\n",
    "    - What device/hardware/configuration are we gonna use for a given application?\n",
    "    - Which sensors and additional data are relevant?\n",
    "    - Who/how is gonna be labelling/labelled the data? Do they need training to standarise the process?\n",
    "    - How much data we need for the application?\n",
    "    - Can we oversee and control the data collection process?\n",
    "    - How can we make data labelling it as easy as possible?\n",
    "    - Can we replicate the training conditions in production?\n",
    "    - **How expensive is it gonna be?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- *Training Data Transport and Processing*:\n",
    "    - Where and how are we gonna store the data?\n",
    "    - How are we gonna transfer data from the devices to our data processing center?\n",
    "    - How much preprocessing we are gonna do on the device (i.e. edge computing)?\n",
    "    - How can we ensure security and privacy (e.g. transport encryption)?\n",
    "    - Have we test the data collection framework properly before data collection starts?\n",
    "    - **What volume of data is expected to flow to the servers per unit of time? Will the infrastructure scale and be robust enough?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- *Data analysis and model building*:\n",
    "    - What do we want to do?\n",
    "    - Which tools/platforms/servers are we gonna use to explore the data?\n",
    "    - What type of data are we studying (e.g. time-series sensor, audio, images, text, etc)?\n",
    "    - What is the dimensionality and structure of the data?\n",
    "    - What are the possible factors that affect to the variance of\n",
    "      the data (e.g. data collection issues or changes in the\n",
    "      environment)?\n",
    "    - How easy will it be?\n",
    "    - Which techniques are more appropiate for a given type and volume of data?\n",
    "    - Can we complement with existing datasets or starting from a pre-trained model?\n",
    "    - How are we gonna to evaluate the performance to have unbiased measures?\n",
    "    - **What is the right trade-off between model complexity and\n",
    "    performance for the given application?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- *Production Environment*:\n",
    "    - How are we gonna be using the resulting model in production?\n",
    "    - Can we deploy it first as a beta or internally to verify that it works as expected?\n",
    "    - Where are we gonna to carry out the model evaluation (e.g. our own remote servers, cloud or device)?\n",
    "    - Can we setup a loop monitoring and redeployment the model in production?\n",
    "    - **How much is expected to be gained by training with more data or improving the model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Collection Infrastructure\n",
    "\n",
    "Here is an scheme of how the IOW data collection infrastructure,\n",
    "that uses common IoT technologies (e.g. a MQTT broker and node-red):\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://raw.githubusercontent.com/pablodecm/datalab_ml_iot/master/04_internet_of_wands/images/iow_infrastructure.png\" height=\"50%\" style=\"max-width: 80%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Collection Campaign\n",
    "\n",
    "Go with your smartphone to https://iow.pablodecm.com/ and generate magic spell demostrations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Downloading Latest Dataset\n",
    "\n",
    "You download the latest dataset from the GitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!wget \"https://github.com/pablodecm/datalab_ml_iot/raw/refs/heads/master/04_internet_of_wands/iow_data.zip?download=\"; unzip -qq -o iow_data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loading the Data\n",
    "\n",
    "We have do decide how we want to represent the data and also\n",
    "work on a custom reader for our set of json-based files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat iow_data/alohomora/Cedric_2579103d.01bba.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "example_file = \"iow_data/wingardium-leviosa/Peppapig_9b2bd7a9.0696f8.json\"\n",
    "\n",
    "example_df = pd.read_json(example_file)\n",
    "print(example_df.dtypes)\n",
    "example_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can apply a series transformation and then invert to get it in a much better format\n",
    "accel_df = example_df.accel_data.apply(pd.Series).T\n",
    "accel_df[\"timestamp\"] = pd.to_timedelta(accel_df.timestamp, unit=\"ms\")\n",
    "accel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timestamps of giro and accel sensors are high precision with a common origin\n",
    "gyro_df = example_df.gyro_data.apply(pd.Series).T\n",
    "gyro_df[\"timestamp\"] = pd.to_timedelta(gyro_df.timestamp, unit=\"ms\")\n",
    "gyro_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use the minimum of timestamp as origin to have the same reference\n",
    "min_timestamp = min([accel_df.timestamp.min(),gyro_df.timestamp.min()])\n",
    "\n",
    "accel_df[\"timestamp\"] -= min_timestamp\n",
    "gyro_df[\"timestamp\"] -= min_timestamp\n",
    "# set first element to 0 to have the same start\n",
    "accel_df.loc[0,\"timestamp\"] = pd.Timedelta(0)\n",
    "gyro_df.loc[0,\"timestamp\"] = pd.Timedelta(0)\n",
    "accel_df = accel_df.set_index(\"timestamp\")\n",
    "gyro_df = gyro_df.set_index(\"timestamp\")\n",
    "\n",
    "accel_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# still the sampling is slightly different\n",
    "gyro_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can resample to fix that\n",
    "resampled_accel_df = accel_df.resample(\"20ms\").mean().interpolate('time')\n",
    "resampled_gyro_df = gyro_df.resample(\"20ms\").mean().interpolate('time')\n",
    "\n",
    "resampled_accel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can then finally merge\n",
    "merged_df = pd.merge(resampled_accel_df, resampled_gyro_df, \n",
    "                     left_index=True, right_index=True,\n",
    "                     suffixes=[\"_accel\",\"_gyro\"], how=\"inner\")\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can not plot the resampled sensor data\n",
    "fig, axs = plt.subplots(2, figsize=(12,12))\n",
    "\n",
    "merged_df.filter(regex=\"._accel\").plot(ax=axs[0])\n",
    "merged_df.filter(regex=\"._gyro\").plot(ax=axs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can define a function to do all the pre-processing for a given json file\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def load_df_from_iow_json(json_path: Path) -> pd.DataFrame:\n",
    "    \n",
    "    # load json file to dataframe\n",
    "    raw_df = pd.read_json(json_path)\n",
    "    \n",
    "    # convert to series to avoid having object elements\n",
    "    accel_df = raw_df.accel_data.apply(pd.Series).T\n",
    "    accel_df[\"timestamp\"] = pd.to_timedelta(accel_df.timestamp, unit=\"ms\")\n",
    "    gyro_df = raw_df.gyro_data.apply(pd.Series).T\n",
    "    gyro_df[\"timestamp\"] = pd.to_timedelta(gyro_df.timestamp, unit=\"ms\")\n",
    "    \n",
    "    # we can use the minimum of timestamp as origin to have the same reference\n",
    "    min_timestamp = min([accel_df.timestamp.min(),gyro_df.timestamp.min()])\n",
    "\n",
    "    # set the same timestamp origin\n",
    "    accel_df[\"timestamp\"] -= min_timestamp\n",
    "    gyro_df[\"timestamp\"] -= min_timestamp\n",
    "    # set first element to 0 to have the same start\n",
    "    accel_df.loc[0,\"timestamp\"] = pd.Timedelta(0)\n",
    "    gyro_df.loc[0,\"timestamp\"] = pd.Timedelta(0)\n",
    "    accel_df = accel_df.set_index(\"timestamp\")\n",
    "    gyro_df = gyro_df.set_index(\"timestamp\")\n",
    "    \n",
    "    # resample and interpolate to make homogeneous\n",
    "    \n",
    "    resampled_accel_df = accel_df.resample(\"20ms\").mean().interpolate('time')\n",
    "    resampled_gyro_df = gyro_df.resample(\"20ms\").mean().interpolate('time')\n",
    "    \n",
    "    # merge in a single df\n",
    "    merged_df = pd.merge(resampled_accel_df, resampled_gyro_df, \n",
    "                         left_index=True, right_index=True,\n",
    "                         suffixes=[\"_accel\",\"_gyro\"], how=\"inner\")\n",
    "    \n",
    "    del raw_df[\"accel_data\"]\n",
    "    del raw_df[\"gyro_data\"]\n",
    "    \n",
    "    return merged_df, raw_df.loc[\"timestamp\"].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if it works from example\n",
    "load_df_from_iow_json(example_file)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and apply fo all dat\n",
    "md_fields = [\"spell_select\",\"device_select\",\"wizard_name\"]\n",
    "data_path = Path(\"./iow_data\")\n",
    "\n",
    "merged_df_dict = {}\n",
    "for path in data_path.glob(\"*/*\"):\n",
    "    rel_path = path.relative_to(data_path)\n",
    "\n",
    "    try:\n",
    "        merged_df, metadata = load_df_from_iow_json(path)\n",
    "    except:\n",
    "        print(f\"Error loading file {rel_path}\")\n",
    "        continue\n",
    "\n",
    "    # ignore data without a without an selected spell\n",
    "    if path.parent.name == \"choose\":\n",
    "        continue\n",
    "        \n",
    "    spell_id = (rel_path.parent/rel_path.stem).as_posix()\n",
    "    key = tuple(metadata.loc[md_fields].to_list() + [spell_id])\n",
    "    merged_df_dict[key] = merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is dataframe with a multi-index (spell_select, device_select, wizard_name, spell_id, timestamp)\n",
    "all_df = pd.concat(merged_df_dict, names = (md_fields+ [\"spell_id\"]))\n",
    "all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorative Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(12,6))\n",
    "\n",
    "var_name = \"y_accel\"\n",
    "wizard_name = \"pablodecm\"\n",
    "spell_name = \"alohomora\"\n",
    "\n",
    "subset_df = all_df.loc[(spell_name,slice(None),wizard_name),var_name]\n",
    "\n",
    "\n",
    "for group_id, group_series in subset_df.groupby(\"spell_id\"):\n",
    "    \n",
    "    group_df = group_series.reset_index()\n",
    "    ax.plot(group_df[\"timestamp\"], group_df[var_name] )\n",
    "\n",
    "ax.set_title(f\"{var_name}-{spell_name}-{wizard_name}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of unique spells in dataset\n",
    "unique_spells = all_df.index.get_level_values(3).unique()\n",
    "print(f\"A total of {len(unique_spells)} spells in dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of examples \n",
    "all_df.groupby([\"spell_select\",\"spell_id\"]).count().groupby(\"spell_select\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,2, figsize=(12,12))\n",
    "\n",
    "summary_vars = all_df.groupby([\"spell_select\",\"spell_id\"]).count()\n",
    "bins = np.linspace(0, 250,41)\n",
    "for plot_i, col_name in enumerate(summary_vars.columns):\n",
    "    ax = axs.flatten()[plot_i]\n",
    "    group_by_spell = summary_vars.loc[:,col_name].groupby(\"spell_select\")\n",
    "    subplots = group_by_spell.plot.hist(ax=ax,alpha=0.8,title=col_name, bins=bins, density=True, histtype='step')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,2, figsize=(12,12))\n",
    "\n",
    "summary_vars = all_df.groupby([\"spell_select\",\"spell_id\"]).mean()\n",
    "\n",
    "for plot_i, col_name in enumerate(summary_vars.columns):\n",
    "    ax = axs.flatten()[plot_i]\n",
    "    group_by_spell = summary_vars.loc[:,col_name].groupby(\"spell_select\")\n",
    "    subplots = group_by_spell.plot.density(ax=ax,alpha=0.5,title=col_name)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feel free to carry out further exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training and Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning a little bit the data\n",
    "# get only spells that last more than 400 ms\n",
    "long_spells = all_df.groupby(\"spell_id\").count() > 20\n",
    "long_spells = long_spells[long_spells].dropna().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and validation datasets\n",
    "train_subset, valid_subset = train_test_split(long_spells, shuffle=True, random_state=7, test_size=0.3)\n",
    "train_df = all_df.loc[(slice(None),slice(None),slice(None),list(train_subset)),:]\n",
    "valid_df = all_df.loc[(slice(None),slice(None),slice(None),list(valid_subset)),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Spell Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sensor sequence we need to create a model that determines the spell cast.\n",
    "There are several ways to frame this multi-class classification problems:\n",
    "- Come up with a clever rule to classify\n",
    "- Manually construct tabular features for each sequence and train a traditional classifier\n",
    "- Use a variable sequence classification model (RNN, transformer)\n",
    "- Other alternative approaches (CNN on image representation of the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a single row (spell) is characterized by 6 sensor recording and variable number of timesteps\n",
    "valid_df.loc[(slice(None),slice(None),slice(None),valid_df.index[0][3])].reset_index(drop=True).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the raw features are the following, however we have the problem that their number is variable\n",
    "features = [ f\"{i}_accel\" for i in [\"x\", \"y\",\"z\"]] + [ f\"{i}_gyro\" for i in [\"x\", \"y\",\"z\"]]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use initially the mean of each wavelenght as a feature\n",
    "train_df_mean = train_df.loc[(slice(None),slice(None),slice(None),slice(None)),:].groupby(\"spell_id\").mean().loc[:, features]\n",
    "train_df_mean.columns = [f\"{f}_mean\" for f in features]\n",
    "valid_df_mean= valid_df.loc[(slice(None),slice(None),slice(None),slice(None)),:].groupby(\"spell_id\").mean().loc[:, features]\n",
    "valid_df_mean.columns = [f\"{f}_mean\" for f in features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can optionally complement with the standard deviation as features\n",
    "train_df_std = train_df.loc[(slice(None),slice(None),slice(None),slice(None)),:].groupby(\"spell_id\").std().loc[:, features]\n",
    "train_df_std.columns = [f\"{f}_std\" for f in features]\n",
    "valid_df_std = valid_df.loc[(slice(None),slice(None),slice(None),slice(None)),:].groupby(\"spell_id\").std().loc[:, features]\n",
    "valid_df_std.columns = [f\"{f}_std\" for f in features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_length = train_df.groupby(\"spell_id\").apply(lambda df: df.shape[0])\n",
    "train_df_length.columns = [\"length\"]\n",
    "valid_df_length = valid_df.groupby(\"spell_id\").apply(lambda df: df.shape[0])\n",
    "valid_df_length.columns = [\"length\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of features\n",
    "train_df_std.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can create a simplified training dataset by contatenating both\n",
    "train_df_extra = pd.concat([train_df_mean,train_df_std, train_df_length], axis=1)\n",
    "valid_df_extra = pd.concat([valid_df_mean,valid_df_std, valid_df_length], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the training data will look like this\n",
    "train_df_extra.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also obtain the category label from the spell_id index\n",
    "label_assign = { \"alohomora\" : 0, \"lumos\" : 1, \"wingardium-leviosa\" : 2, \"reparo\" : 3}\n",
    "train_y =  train_df_extra.reset_index().spell_id.str.split(\"/\").apply(lambda x: label_assign[x[0]])\n",
    "valid_y =  valid_df_extra.reset_index().spell_id.str.split(\"/\").apply(lambda x: label_assign[x[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_clf = GradientBoostingClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can train the classifier\n",
    "gb_clf.fit(train_df_extra, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can get probability predictions\n",
    "gb_clf.predict_proba(train_df_extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and compute some metrics on the training datataset (not to be trusted)\n",
    "y_train_clf_proba = gb_clf.predict_proba(train_df_extra)[:, 1]\n",
    "y_train_clf_pred = gb_clf.predict(train_df_extra)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(train_y,y_train_clf_pred))\n",
    "print(\"Gradient Boosting Classifier Accuracy: \"+\"{:.1%}\".format(accuracy_score(train_y,y_train_clf_pred)));\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(train_y,y_train_clf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_clf_proba = gb_clf.predict_proba(valid_df_extra)[:, 1]\n",
    "y_valid_clf_pred = gb_clf.predict(valid_df_extra)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(valid_y,y_valid_clf_pred))\n",
    "print(\"Gradient Boosting Classifier Accuracy: \"+\"{:.1%}\".format(accuracy_score(valid_y,y_valid_clf_pred)));\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(valid_y,y_valid_clf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can try to do a grid search to try to find a better hyper-parameter combination\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_clf = GradientBoostingClassifier()\n",
    "\n",
    "# to avoid having same UnitNumber in both sets\n",
    "cv = KFold(3)\n",
    "\n",
    "param_grid = { \"n_estimators\" : [100, 130, 150, 180, 200],\n",
    "               \"learning_rate\" :  [0.05, .1, 0.07]\n",
    "             }\n",
    "\n",
    "\n",
    "\n",
    "optimized_gb_clf = GridSearchCV(estimator=gb_clf,\n",
    "                            cv = cv,\n",
    "                            param_grid=param_grid,\n",
    "                            verbose = 1,\n",
    "                            n_jobs = -1)\n",
    "\n",
    "# we train the best model with the full dataset\n",
    "optimized_gb_clf.fit(train_df_extra, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute again the metrics on the validation set\n",
    "y_valid_clf_proba = optimized_gb_clf.predict_proba(valid_df_extra)[:, 1]\n",
    "y_valid_clf_pred = optimized_gb_clf.predict(valid_df_extra)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(valid_y,y_valid_clf_pred))\n",
    "print(\"Gradient Boosting Classifier Accuracy: \"+\"{:.1%}\".format(accuracy_score(valid_y,y_valid_clf_pred)));\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(valid_y,y_valid_clf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "model = optimized_gb_clf.best_estimator_\n",
    "joblib.dump(model,\"optimized_gb_clf.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_y != y_valid_clf_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is possible that some of the mistakes are due to\n",
    "# incorrect data, we could potentially explore some of the \n",
    "wrongly_classified_ids = (valid_df_extra.loc[(valid_y != y_valid_clf_pred).values]).index\n",
    "wrongly_classified_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrongly_classified_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could check a few of the incorrectly classified to see \n",
    "spell_id = wrongly_classified_ids[10]\n",
    "valid_df.loc[(slice(None),slice(None),slice(None), spell_id)].reset_index(drop=True).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: train a random forest model or other models with sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feel free to carry out additonal work and/or train additional models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About high-resolution time in web applications:\n",
    "\n",
    "https://www.w3.org/TR/hr-time-2/#dfn-time-origin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For an overview of a state-of-the-art distributed training\n",
    "infrastructure including redeployment and the importance\n",
    "of edge in real-time applications you can check the Tesla Autonomy day presentation:\n",
    "\n",
    "- [1] [*Tesla Autonomy Day*](https://www.youtube.com/watch?v=Ucp0TTmvqOE)  Youtube video (+2hrs) or newer Tesla AI day\n",
    "\n",
    "There are several publications using combinations of RNNs and CNNs\n",
    "for dealing with IoT sensor data, for task such as\n",
    "Human Activity Recognition:\n",
    "- [2] Yao, Shuochao, et al. [*Deepsense: A unified deep learning framework for time-series mobile sensing data processing*](https://arxiv.org/abs/1611.01942) Proceedings of the 26th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee, 2017.\n",
    "\n",
    "Many more recent using transformers and/or semi-supervised approaches."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "rise": {
   "theme": "black"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
